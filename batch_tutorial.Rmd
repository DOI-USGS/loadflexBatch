---
title: "Loadflex Batch Mode"
date: '`r format(Sys.time(), "%d %B, %Y")`'
author: "David Watkins and Alison Appling"
output:
  html_document: 
    highlight: tango
    theme: sandstone
    css: newfeatures.css
    toc: true
    toc_float: true
  md_document:
    variant: markdown_github
slug: loadflex-batch
---
```{r setup, include=FALSE}
library(knitr)

knit_hooks$set(plot=function(x, options) {
  sprintf(
    "<img src='/%s%s-%d.%s'/>", 
    options$fig.path, options$label, options$fig.cur, options$fig.ext)
})

opts_chunk$set(
  echo=TRUE,
  fig.path="static/loadflex-batch/"
)
```

## New features

Features introduced since May 2017 are <span class="new">colored like this</span>.

## Loadflex batch script

The script at https://github.com/USGS-R/loadflexBatch/blob/master/batch.R automates running multiple load models for multiple sites and consitutents, using the `loadflex` package along with some features of `rloadest` <span class="new">and an R implementation of Beale's Ratio Estimator by Lillian Gorman-Sanisaca (based on Greg Schwarz's SAS code)</span>.  Model output is collated across all sites for easy analysis of input data, predicted loads, and model metrics.  This post will go through basic setup and use of the script.

## Installation and setup

First, go to the Github repository [USGS-R/loadflexBatch](https://github.com/USGS-R/loadflexBatch) and download the zip file (using the big green "Clone or Download" button and the "Download ZIP" option within). Move the zip file to your preferred directory for R projects, then unzip it. Open RStudio, start a new project (File -> New Project), select the "Existing directory" option, select the `loadflexBatch-master` folder, and click "Create Project". Note that on Windows there will be two `loadflexBatch-master` folders â€” select the inner one. You will now be inside the `loadflexBatch-master` folder, and have access to the batch script.
```{r} 
list.files()
```
  
Next, install the packages that the script depends on.  In your console, run 
```{r eval = FALSE}
install.packages(
  c("devtools", "car", "dplyr", "ggplot2", "lubridate", "MASS", "Matrix", "yaml"),
  dependencies=TRUE, type="both")
install.packages(
  c("smwrData", "smwrBase", "smwrGraphs", "smwrStats", "smwrQW", "rloadest", "unitted"), 
  repos=c("https://owi.usgs.gov/R", "https://cran.rstudio.com"), 
  dependencies=TRUE, type="both")
```

Install the main `loadflex` package directly from Github to ensure you have the very latest version:
```{r eval=FALSE}
devtools::install_github("USGS-R/loadflex")
packageVersion('loadflex')
```
The most up-to-date installation instructions for `loadflex` can always be found at [https://github.com/USGS-R/loadflex#installation](https://github.com/USGS-R/loadflex#installation).

Now you are ready to look at the user inputs, run the script, and inspect the output.

## Inputs

### Data files

Your inputs should be contained within a single folder that contains subfolders for each constituent, a subfolder for discharge, and a `siteInfo.csv` file. In the following example, the main folder ("input") has subfolders for two constituents ("NO3","PT") and discharge ("Q"), and each subfolder contains one file for each monitoring site.

```
- input
  - NO3
    - RONC02800.csv
    - MOGU02900.csv
    - ORIZ02800.csv
  - PT
    - RONC02800.csv
    - MOGU02900.csv
    - ORIZ02800.csv
  - Q
    - RONC02800.csv
    - MOGU02900.csv
    - ORIZ02800.csv
  - siteInfo.csv
```

Below is an example of a consituent data file, a comma-separated table (.csv file) with columns for date of the observation (`date`), discharge (`Q`), the concentration of the constituent (in this case `NO3`), additional columns that are ignored by the batch script (e.g., `CODIGO_ESTACAO`), and the censoring and data quality code (`status`), which follows the Brazillian ANA's convention of 0=bad value, 1=normal value, 2=value known to be less than or equal to the number given in the constituent (`NO3`) column. The names of the constituent and discharge columns must be exactly equal to the folder names for that constituent and discharge.
```{r}
head(read.csv('three_ANA_sites/input/NO3/MOGU02900.csv'), 5)
```

Next is an example of a discharge data file, with columns for date of the observation (`date`) and mean daily discharge (`Q`), plus optional additional columns that will be ignored by the batch script. The name of the discharge column must be exactly equal to the folder name for discharge. Whereas the constituent data file only has rows for those dates on which concentration was measured, the discharge data file has rows for every date on which flux is to be estimated.
```{r}
head(read.csv('three_ANA_sites/input/Q/MOGU02900.csv'), 5)
```

### siteInfo file

The file called `siteInfo.csv` contains a table of metadata for individual water quality and discharge sites. Here is the file in our "three_ANA_sites" example:
```{r}
read.csv('three_ANA_sites/input/siteInfo.csv')
```

The values in the `site.id` column are the names (without extensions) of the .csv files within each constituent or discharge folder. The `site.name` column can contain a more human-readable site name. `lat` and `lon` give the coordinates of each site in degrees North and East, respectively.

An important function of the `siteInfo` file is to match up input files describing constituent concentrations and river discharges. Water quality and discharge are usually measured at the exact same location, but sometimes water quality is sampled somewhat downstream or upstream of the flow gage. The script handles this possible discrepancy by expecting two columns for site identifiers in the `siteInfo` file. The column called `site.id` describes precise site locations. If water quality is measured downstream or upstream of the flow gage, the concentration and discharge rows in the `siteInfo` file should have different `site.id` values. The column called `matching.site` is used to match up pairs of water quality and discharge sites that you want to combine into a load model. The `matching.site` should always be the same for the concentration row and discharge row to be combined. Once combined, discharge will be scaled according to `basin.area` to estimate what discharge would have been if measured exactly at the water quality monitoring site. 

The values in the `constituent` column each name a constituent or discharge folder within the main input folder ("NO3", "PT", and "Q" in this example). Including discharge as a "constituent" makes this file more consise. The `units` column gives the units of measurement for each constituent-site combination.

### Configuration file

The final input file, the "configuration file", lives outside your main input folder and provides high-level information about your inputs and desired outputs. Examples in the `loadflexBatch` project are `three_ANA_sites.yml` and `Hirsch_sites.yml`. The configuration file is in the YAML language ([http://www.yaml.org/](http://www.yaml.org/)). In YAML, each line contains a `key: value` pair, except blank lines and those lines beginning with `#`, which are comments. In your user input YAML file you should supply information about input/output folder names and locations, constituents, load units, and load rate units. `three_ANA_sites.yml` looks like this:

<pre>
# This YAML file contains high-level user options for running batch.R on a
# specific dataset. YAML files are made of key: value pairs like the ones below.
# Edit the values (the text to the right of each colon) to describe your data.

# Input files
<span class="yml">inputFolder: "three_ANA_sites/input"</span> # folder containing all input files and subfolders
<span class="yml">constituents: ["NO3", "PT"]</span> # names of folders inside inputFolder containing constituent data, and the column names for constituents within those data files
<span class="yml">discharge: "Q"</span> # name of the folder inside inputFolder containing daily discharge data, and the column name for discharge within those data files
<span class="yml">date: "date"</span> # column name for dates within the constituent and discharge data files
<span class="yml">siteInfo: "siteInfo.csv"</span> # name of the csv file inside inputFolder containing site and constituent metadata

# Analysis specifications
<span class="new">models: ["RL5","RL7","CMP","INT","BRE"]</span> # models to run; choose any/all of ["RL5","RL7","CMP","INT","BRE"]
<span class="yml">minDaysPerYear: 345</span> # number of days required for a year to be included in the multi-year average
<span class="new">regMaxNaNsPerYear: 0</span> # affects rloadest models only: max NaN days allowed for including year in annual or multi-year loads; estimates with NaNs can be very slow

# Units for output
<span class="yml">loadUnits: "kg"</span>
<span class="yml">loadRateUnits: "kg/yr"</span>

# Output folder
<span class="yml">outputFolder: "three_ANA_sites/output" # folder where results files and subfolders will be written
<span class="new">outputTimestamp: FALSE</span> # if TRUE, output folder appended with DATE_TIME
</pre>

The configuration file is broken into four sections: "Input files", "Analysis specifications", "Units for output", and "Output folder". The comments on each line provide some information about each parameter, but below we can provide a little more:

#### Input files

* `inputFolder` - the file path of your folder containing all input files besides the configuration file. If you're using an RStudio project for this script and your data files, this file path can be relative to your RStudio project folder.
* `constituents` - the names of the input subfolders that contain the water quality data. This list may omit some of the constituent folders actually present in your input data folder. Only those folders mentioned in this list will be processed during the batch run, even if they exist in the input data folder and/or the `siteInfo` folder. As noted above in the **Data files** section of this tutorial, the folder names must also exactly match the column name in each data file containing constituent concentrations.
* `discharge` - the name of the folder containing discharge data files. As noted above in the **Data files** section of this tutorial, the folder name must also exactly match the column name in each data file containing discharge rates.
* `date` - the column name for dates within the constituents and discharge data files.
* `siteInfo` - the file name of a comma-separated (.csv) file inside `inputFolder`, as described in the **siteInfo file** section of this tutorial.

#### Analysis specifications
* <span class="new">`models`</span> - a list of the models you want to fit during the batch run. There are five options, of which you can choose any or all.
    - RL5: 5-parameter `rloadest` regression model, with terms for Q, T, sin(T), cos(T), and an intercept
    - <span class="new">RL7</span>: 7-parameter `rloadest` regression model, adding terms for Q^2 and T^2
    - CMP: composite model combining RL5 with a rectangular interpolation of concentration residuals
    - INT: rectangular interpolation of concentrations
    - <span class="new">BRE</span>: Beale's ratio estimator
* `minDaysPerYear` - minimum number of complete days for a year to be considered "complete", such that annual and multi-year estimates will include this year
* <span class="new">`regMaxNaNsPerYear`</span> - `rloadest` models can be really slow at producing annual and multi-year estimates; the slowness occurs when any of the daily predictions come out as `NaN`. This number gives the maximum number of daily `NaN`s we'll permit before giving up on a year and skipping it for both annual and multi-year estimates.

#### Units for output
* `loadUnits` - the units of load predictions. We haven't tested much besides "kg", so that's what we recommend.
* `loadRateUnits` - the units of load predictions. We haven't tested much besides "kg/yr", so that's what we recommend.

#### Output folder
* `outputFolder` - the file path of a folder where you want output files to be written. As with `inputFolder`, this can be a relative path if you're using an RStudio project. See the **Output files** section of this tutorial, below.
* <span class="new">`outputTimestamp`</span> - TRUE or FALSE; if TRUE, each time you run batch mode, a new output folder will automatically be created. The name of that output folder will be a combination of `outputFolder` and a date-time stamp, so you'll know when the batch was run. Setting this option to TRUE ensures that you will never overwrite existing output files.

### Remember

When preparing your files, remember that IDs for constituents, discharge, and sites need to be consistent across the configuration file, the `siteInfo` file, column names within the data files, and the folder and file names in the `siteInputs` folder.


## Running the script

Open the batch script, `batch.R`, by clicking on it in the 'Files' pane in the lower right of your RStudio window.  There is a basic description of the file at the top. Below that is a command to read in the configuration file for one of the included example datasets. It looks like this:
```{r eval=FALSE}
inputs <- yaml::yaml.load_file('three_ANA_sites.yml')
```

The default configuration file is `three_ANA_sites.yml`. To run another example (e.g., `Hirsch_sites.yml`) or your own file, edit the file name in the above line of the `batch.R` script. 

Once your configuration file is named, you can source the script. Find the "Source" button in the top right of the text editor where you're viewing the script, click the drop-down error, and choose "Source". You'll see a whole lot of package startup messages:
```{r eval = FALSE, code_folding='hide'}
Attaching package: â€˜dplyrâ€™

The following objects are masked from â€˜package:statsâ€™:

    filter, lag

The following objects are masked from â€˜package:baseâ€™:

    intersect, setdiff, setequal, union

Loading required package: rloadest
Loading required package: smwrBase
Loading required package: lubridate

Attaching package: â€˜lubridateâ€™

The following object is masked from â€˜package:baseâ€™:

    date


Attaching package: â€˜smwrBaseâ€™

The following objects are masked from â€˜package:dplyrâ€™:

    coalesce, recode

Loading required package: smwrGraphs
This information is preliminary or provisional and is subject to revision. It is being provided to meet the need for timely best science. The information has not received final approval by the U.S. Geological Survey (USGS) and is provided on the condition that neither the USGS nor the U.S. Government shall be held liable for any damages resulting from the authorized or unauthorized use of the information. Although this software program has been used by the USGS, no warranty, expressed or implied, is made by the USGS or the U.S. Government as to the accuracy and functioning of the program and related program material nor shall the fact of distribution constitute any such warranty, and no responsibility is assumed by the USGS in connection therewith.
Loading required package: smwrStats
Although this software program has been used by the U.S. Geological Survey (USGS), no warranty, expressed or implied, is made by the USGS or the U.S. Government as to the accuracy and functioning of the program and related program material nor shall the fact of distribution constitute any such warranty, and no responsibility is assumed by the USGS in connection therewith.
Loading required package: smwrQW
Loading required package: dataRetrieval
This information is preliminary or provisional and is subject to revision. It is being provided to meet the need for timely best science. The information has not received final approval by the U.S. Geological Survey (USGS) and is provided on the condition that neither the USGS nor the U.S. Government shall be held liable for any damages resulting from the authorized or unauthorized use of the information. Although this software program has been used by the USGS, no warranty, expressed or implied, is made by the USGS or the U.S. Government as to the accuracy and functioning of the program and related program material nor shall the fact of distribution constitute any such warranty, and no responsibility is assumed by the USGS in connection therewith.

Attaching package: â€˜smwrQWâ€™

The following object is masked from â€˜package:utilsâ€™:

    View

Although this software program has been used by the U.S. Geological Survey (USGS), no warranty, expressed or implied, is made by the USGS or the U.S. Government as to the accuracy and functioning of the program and related program material nor shall the fact of distribution constitute any such warranty, and no responsibility is assumed by the USGS in connection therewith.
```
Then you'll see text appear as the batch script proceeds through the constituents and sites you've asked to model.
```{r eval=FALSE, code_folding='hide'}
running loadflex version 1.1.11 in batch mode at 2017-07-06 18:52:58
processing NO3 at site RONC02800 with files
 three_ANA_sites/input/NO3/RONC02800.csv and
 three_ANA_sites/input/Q/RONC02800.csv
You are fitting an rloadest model (loadReg). Please remember to cite both citation('loadflex') and citation('rloadest').
 * generating annual mean load estimates...
   RL5...|==========================================================================================================        | 93% ~0 s remaining     
   RL7...skipping NaN-riddled 2001...skipping NaN-riddled 2002...skipping NaN-riddled 2003...skipping NaN-riddled 2004...skipping NaN-riddled 2005...skipping NaN-riddled 2006...skipping NaN-riddled 2007...   CMP...   INT...   BRE...done!
 * generating multi-year mean load estimates...RL5...RL7...CMP...INT...BRE...done!
processing NO3 at site MOGU02900 with files
 three_ANA_sites/input/NO3/MOGU02900.csv and
 three_ANA_sites/input/Q/MOGU02900.csv
 * generating annual mean load estimates...
   RL5...|==================================================================================================================|100% ~0 s remaining     
   RL7...|==========================================================================================================        | 93% ~0 s remaining     
   CMP...   INT...   BRE...done!
 * generating multi-year mean load estimates...RL5...RL7...CMP...INT...BRE...done!
processing NO3 at site ORIZ02900 with files
 three_ANA_sites/input/NO3/ORIZ02900.csv and
 three_ANA_sites/input/Q/ORIZ02900.csv
 * scaling discharge by basin area: multiplying by 0.857
 * generating annual mean load estimates...
   RL5...|==================================================================================================================|100% ~0 s remaining     
   RL7...|==================================================================================================================|100% ~0 s remaining     
   CMP...   INT...   BRE...done!
 * generating multi-year mean load estimates...RL5...RL7...CMP...INT...BRE...done!
processing PT at site RONC02800 with files
 three_ANA_sites/input/PT/RONC02800.csv and
 three_ANA_sites/input/Q/RONC02800.csv
 * generating annual mean load estimates...
   RL5...|==========================================================================================================        | 93% ~0 s remaining     
   RL7...skipping NaN-riddled 2001...skipping NaN-riddled 2002...skipping NaN-riddled 2003...skipping NaN-riddled 2004...skipping NaN-riddled 2005...skipping NaN-riddled 2006...skipping NaN-riddled 2007...|==================================================================================================================|100% ~0 s remaining     
   CMP...   INT...   BRE...done!
 * generating multi-year mean load estimates...RL5...RL7...CMP...INT...BRE...done!
processing PT at site MOGU02900 with files
 three_ANA_sites/input/PT/MOGU02900.csv and
 three_ANA_sites/input/Q/MOGU02900.csv
 * generating annual mean load estimates...
   RL5...|==========================================================================================================        | 93% ~0 s remaining     
   RL7...|==================================================================================================================|100% ~0 s remaining     
   CMP...   INT...   BRE...done!
 * generating multi-year mean load estimates...RL5...RL7...CMP...INT...BRE...done!
processing PT at site ORIZ02900 with files
 three_ANA_sites/input/PT/ORIZ02900.csv and
 three_ANA_sites/input/Q/ORIZ02900.csv
 * scaling discharge by basin area: multiplying by 0.857
 * generating annual mean load estimates...
   RL5...|==========================================================================================================        | 93% ~0 s remaining     
   RL7...|==========================================================================================================        | 93% ~0 s remaining     
   CMP...   INT...   BRE...done!
 * generating multi-year mean load estimates...RL5...RL7...CMP...INT...BRE...done!
the NO3 inputs summary has been written to three_ANA_sites/output/NO3/NO3_inputs.csv
the PT inputs summary has been written to three_ANA_sites/output/PT/PT_inputs.csv
the NO3 annual summary has been written to three_ANA_sites/output/NO3/NO3_annual.csv
the PT annual summary has been written to three_ANA_sites/output/PT/PT_annual.csv
the NO3 multiYear summary has been written to three_ANA_sites/output/NO3/NO3_multiYear.csv
the PT multiYear summary has been written to three_ANA_sites/output/PT/PT_multiYear.csv
the NO3 modelMetrics summary has been written to three_ANA_sites/output/NO3/NO3_modelMetrics.csv
the PT modelMetrics summary has been written to three_ANA_sites/output/PT/PT_modelMetrics.csv
use Adobe or equivalent to combine pdfs into CONST_plots.pdf
Warning messages:
1: In sqrt(out.data$loadvar) : NaNs produced
2: In log(Flux[KDays]) : NaNs produced
3: In sqrt(out.data$loadvar) : NaNs produced
4: In log(Flux[KDays]) : NaNs produced
```
And done!

## Behind the scenes

The script reads, processes, and writes output for each site/consituent combination individually. There are five available models. The <span class="new">two</span> regression models, `RL5` and <span class="new">`RL7`</span>, are implemented within the `rloadest` R package. The composite (`CMP`) and interpolation (`INT`) models are implemented within the `loadflex` package. `loadflex` provides a consistent interface to all four of these models. <span class="new">The fifth model, Beale's Ratio Estimator (`BRE`), is temporarily implemented as a handful of scripts within this `loadflexBatch` project - see the `batch_Beales` folder.</span>

If a constituent dataset includes censored data (i.e., `status` flags of 2), those data are passed to the `loadReg2` models in the censored data format required by `rloadest`. The composite and interpolation models and Beale's Ratio Estimator all use the crude approximation that a censored value is equal to 1/2 of the reported detection limit; e.g., a censored value of `<0.8` will be treated as `0.4` in these three models.

The script uses several built-in `loadflex` functions to create the outputs in R, which the script then writes to files. Those functions include `summarizeInputs`, `summarizeModel`, and `predictSolute`. Descriptions of these functions can be found by typing `?` followed by the function name at the R prompt.


## Output files

Like the input files, the output files are written to a separate folder for each constituent. Most important are the four summary files, each of which covers all the sites for that consituent. These are prefixed with the constituent name, so for "NO3", these are:

```
- output
  - NO3
    - NO3_inputs.csv       # input data summary
    - NO3_modelMetrics.csv # diagnostics and descriptive metrics for each model
    - NO3_annual.csv       # annual predicted loads from each model
    - NO3_multiYear.csv    # multi-year predicted loads from each model
```
      
Additionally, there are five folders containing individual .csv files for each site. The four .csv files listed above combine the contents of four of those folders. The folders are:

```
- output
  - NO3
    - inputs               # input data, one summary file per site
    - modelMetrics         # diagnostics and descriptive metrics, one summary file per site
    - annual               # annual predicted loads, one summary file per site
    - multiYear            # multi-year predicted loads, one summary file per site
    - plots                # diagnostic plots, one .pdf file per site
```

You might want to produce one pdf file containing all the plots for all sites. This is hard to do in R but easy if you have Adobe Acrobat Pro or a similar pdf-managing program (some are even available as free online services). With Acrobat, simply open your RStudio project folder in a regular file explorer, select all the files in the `output/NO3/plots` folder, right click, and choose "Combine files in Acrobat...". We recommend that you create a summary file that follows the naming convention of the first four summary files, e.g.:
```
- output
  - NO3
    - NO3_plots.pdf        # plots of input data, predicted loads, and model diagnostics
```

## What's next?

Create your own input files following the conventions above. Edit line `r grep("inputs <- yaml::yaml.load_file", readLines('batch.R'))` of `batch.R` to point to your own configuration YAML file, e.g.,
```{r eval=FALSE}
inputs <- yaml::yaml.load_file('my_own_sites.yml')
```
Then click Source to produce outputs for your sites.

Inspect the output files to learn about the size and quality of the input data, how well each model performed, and how the model predictions varied across models and sites. You're on your way!
